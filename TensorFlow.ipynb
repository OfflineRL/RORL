{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import d4rl\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "ENV_NAME = 'kitchen-complete-v0'\n",
    "env = gym.make(ENV_NAME)\n",
    "render = lambda : plt.imshow(env.render(mode='rgb_array'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.reset())\n",
    "render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "print(action)\n",
    "print(env.step(action))\n",
    "render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPG implementation\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import gym\n",
    "import time\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def count_vars(module):\n",
    "    return sum([np.prod(p.shape) for p in module.parameters()])\n",
    "\n",
    "class MLPActor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):\n",
    "        super().__init__()\n",
    "        pi_sizes = [obs_dim] + list(hidden_sizes) + [act_dim]\n",
    "        self.pi = mlp(pi_sizes, activation, nn.Tanh)\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Return output from network scaled to action space limits.\n",
    "        return self.act_limit * self.pi(obs)\n",
    "\n",
    "class MLPQFunction(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        q = self.q(torch.cat([obs, act], dim=-1))\n",
    "        return torch.squeeze(q, -1) # Critical to ensure q has right shape.\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, observation_space, action_space, hidden_sizes=(256,256),\n",
    "                 activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_dim = observation_space.shape[0]\n",
    "        act_dim = action_space.shape[0]\n",
    "        act_limit = action_space.high[0]\n",
    "\n",
    "        # build policy and value functions\n",
    "        self.pi = MLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit)\n",
    "        self.q = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation)\n",
    "\n",
    "    def act(self, obs):\n",
    "        with torch.no_grad():\n",
    "            return self.pi(obs).numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple FIFO experience replay buffer for DDPG agents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        batch = dict(obs=self.obs_buf[idxs],\n",
    "                     obs2=self.obs2_buf[idxs],\n",
    "                     act=self.act_buf[idxs],\n",
    "                     rew=self.rew_buf[idxs],\n",
    "                     done=self.done_buf[idxs])\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}\n",
    "\n",
    "\n",
    "\n",
    "def ddpg(env_fn, actor_critic=MLPActorCritic, ac_kwargs=dict(), seed=0, \n",
    "         steps_per_epoch=1000, epochs=100, replay_size=int(1e6), gamma=0.99, \n",
    "         polyak=0.995, pi_lr=1e-3, q_lr=1e-3, batch_size=100, start_steps=10000, \n",
    "         update_after=1000, update_every=50, act_noise=0.1, num_test_episodes=10, \n",
    "         max_ep_len=1000, logger_kwargs=dict(), save_freq=1):\n",
    "    \"\"\"\n",
    "    Deep Deterministic Policy Gradient (DDPG)\n",
    "    Args:\n",
    "        env_fn : A function which creates a copy of the environment.\n",
    "            The environment must satisfy the OpenAI Gym API.\n",
    "        actor_critic: The constructor method for a PyTorch Module with an ``act`` \n",
    "            method, a ``pi`` module, and a ``q`` module. The ``act`` method and\n",
    "            ``pi`` module should accept batches of observations as inputs,\n",
    "            and ``q`` should accept a batch of observations and a batch of \n",
    "            actions as inputs. When called, these should return:\n",
    "            ===========  ================  ======================================\n",
    "            Call         Output Shape      Description\n",
    "            ===========  ================  ======================================\n",
    "            ``act``      (batch, act_dim)  | Numpy array of actions for each \n",
    "                                           | observation.\n",
    "            ``pi``       (batch, act_dim)  | Tensor containing actions from policy\n",
    "                                           | given observations.\n",
    "            ``q``        (batch,)          | Tensor containing the current estimate\n",
    "                                           | of Q* for the provided observations\n",
    "                                           | and actions. (Critical: make sure to\n",
    "                                           | flatten this!)\n",
    "            ===========  ================  ======================================\n",
    "        ac_kwargs (dict): Any kwargs appropriate for the ActorCritic object \n",
    "            you provided to DDPG.\n",
    "        seed (int): Seed for random number generators.\n",
    "        steps_per_epoch (int): Number of steps of interaction (state-action pairs) \n",
    "            for the agent and the environment in each epoch.\n",
    "        epochs (int): Number of epochs to run and train agent.\n",
    "        replay_size (int): Maximum length of replay buffer.\n",
    "        gamma (float): Discount factor. (Always between 0 and 1.)\n",
    "        polyak (float): Interpolation factor in polyak averaging for target \n",
    "            networks. Target networks are updated towards main networks \n",
    "            according to:\n",
    "            .. math:: \\\\theta_{\\\\text{targ}} \\\\leftarrow \n",
    "                \\\\rho \\\\theta_{\\\\text{targ}} + (1-\\\\rho) \\\\theta\n",
    "            where :math:`\\\\rho` is polyak. (Always between 0 and 1, usually \n",
    "            close to 1.)\n",
    "        pi_lr (float): Learning rate for policy.\n",
    "        q_lr (float): Learning rate for Q-networks.\n",
    "        batch_size (int): Minibatch size for SGD.\n",
    "        start_steps (int): Number of steps for uniform-random action selection,\n",
    "            before running real policy. Helps exploration.\n",
    "        update_after (int): Number of env interactions to collect before\n",
    "            starting to do gradient descent updates. Ensures replay buffer\n",
    "            is full enough for useful updates.\n",
    "        update_every (int): Number of env interactions that should elapse\n",
    "            between gradient descent updates. Note: Regardless of how long \n",
    "            you wait between updates, the ratio of env steps to gradient steps \n",
    "            is locked to 1.\n",
    "        act_noise (float): Stddev for Gaussian exploration noise added to \n",
    "            policy at training time. (At test time, no noise is added.)\n",
    "        num_test_episodes (int): Number of episodes to test the deterministic\n",
    "            policy at the end of each epoch.\n",
    "        max_ep_len (int): Maximum length of trajectory / episode / rollout.\n",
    "        save_freq (int): How often (in terms of gap between epochs) to save\n",
    "            the current policy and value function.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    env, test_env = env_fn(), env_fn()\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "    # Action limit for clamping: critically, assumes all dimensions share the same bound!\n",
    "    act_limit = env.action_space.high[0]\n",
    "\n",
    "    # Create actor-critic module and target networks\n",
    "    ac = actor_critic(env.observation_space, env.action_space, **ac_kwargs)\n",
    "    ac_targ = deepcopy(ac)\n",
    "\n",
    "    # Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
    "    for p in ac_targ.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Experience buffer\n",
    "    replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)\n",
    "\n",
    "    # Count variables (protip: try to get a feel for how different size networks behave!)\n",
    "    var_counts = tuple(count_vars(module) for module in [ac.pi, ac.q])\n",
    "    print('\\nNumber of parameters: \\t pi: %d, \\t q: %d\\n'%var_counts)\n",
    "\n",
    "    # Set up function for computing DDPG Q-loss\n",
    "    def compute_loss_q(data):\n",
    "        o, a, r, o2, d = data['obs'], data['act'], data['rew'], data['obs2'], data['done']\n",
    "\n",
    "        q = ac.q(o,a)\n",
    "\n",
    "        # Bellman backup for Q function\n",
    "        with torch.no_grad():\n",
    "            q_pi_targ = ac_targ.q(o2, ac_targ.pi(o2))\n",
    "            backup = r + gamma * (1 - d) * q_pi_targ\n",
    "\n",
    "        # MSE loss against Bellman backup\n",
    "        loss_q = ((q - backup)**2).mean()\n",
    "\n",
    "        # Useful info for logging\n",
    "        loss_info = dict(QVals=q.detach().numpy())\n",
    "\n",
    "        return loss_q, loss_info\n",
    "\n",
    "    # Set up function for computing DDPG pi loss\n",
    "    def compute_loss_pi(data):\n",
    "        o = data['obs']\n",
    "        q_pi = ac.q(o, ac.pi(o))\n",
    "        return -q_pi.mean()\n",
    "\n",
    "    # Set up optimizers for policy and q-function\n",
    "    pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "    q_optimizer = Adam(ac.q.parameters(), lr=q_lr)\n",
    "\n",
    "\n",
    "    def update(data):\n",
    "        # First run one gradient descent step for Q.\n",
    "        q_optimizer.zero_grad()\n",
    "        loss_q, loss_info = compute_loss_q(data)\n",
    "        loss_q.backward()\n",
    "        q_optimizer.step()\n",
    "\n",
    "        # Freeze Q-network so you don't waste computational effort \n",
    "        # computing gradients for it during the policy learning step.\n",
    "        for p in ac.q.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Next run one gradient descent step for pi.\n",
    "        pi_optimizer.zero_grad()\n",
    "        loss_pi = compute_loss_pi(data)\n",
    "        loss_pi.backward()\n",
    "        pi_optimizer.step()\n",
    "\n",
    "        # Unfreeze Q-network so you can optimize it at next DDPG step.\n",
    "        for p in ac.q.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        # Record things\n",
    "        tqdm.write(f\"Loss Q {loss_q.item()}, LossPi={loss_pi.item()}\")\n",
    "\n",
    "        # Finally, update target networks by polyak averaging.\n",
    "        with torch.no_grad():\n",
    "            for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
    "                # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "                # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "                p_targ.data.mul_(polyak)\n",
    "                p_targ.data.add_((1 - polyak) * p.data)\n",
    "\n",
    "    def get_action(o, noise_scale):\n",
    "        a = ac.act(torch.as_tensor(o, dtype=torch.float32))\n",
    "        a += noise_scale * np.random.randn(act_dim)\n",
    "        return np.clip(a, -act_limit, act_limit)\n",
    "\n",
    "    def test_agent():\n",
    "        for j in range(num_test_episodes):\n",
    "            o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0\n",
    "            while not(d or (ep_len == max_ep_len)):\n",
    "                # Take deterministic actions at test time (noise_scale=0)\n",
    "                o, r, d, _ = test_env.step(get_action(o, 0))\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "\n",
    "    # Prepare for interaction with environment\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    start_time = time.time()\n",
    "    o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    for t in tqdm(range(total_steps)):\n",
    "        \n",
    "        # Until start_steps have elapsed, randomly sample actions\n",
    "        # from a uniform distribution for better exploration. Afterwards, \n",
    "        # use the learned policy (with some noise, via act_noise). \n",
    "        if t > start_steps:\n",
    "            a = get_action(o, act_noise)\n",
    "        else:\n",
    "            a = env.action_space.sample()\n",
    "\n",
    "        # Step the env\n",
    "        o2, r, d, _ = env.step(a)\n",
    "        ep_ret += r\n",
    "        ep_len += 1\n",
    "\n",
    "        # Ignore the \"done\" signal if it comes from hitting the time\n",
    "        # horizon (that is, when it's an artificial terminal signal\n",
    "        # that isn't based on the agent's state)\n",
    "        d = False if ep_len==max_ep_len else d\n",
    "\n",
    "        # Store experience to replay buffer\n",
    "        replay_buffer.store(o, a, r, o2, d)\n",
    "\n",
    "        # Super critical, easy to overlook step: make sure to update \n",
    "        # most recent observation!\n",
    "        o = o2\n",
    "\n",
    "        # End of trajectory handling\n",
    "        if d or (ep_len == max_ep_len):\n",
    "            o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "        # Update handling\n",
    "        if t >= update_after and t % update_every == 0:\n",
    "            for _ in range(update_every):\n",
    "                batch = replay_buffer.sample_batch(batch_size)\n",
    "                update(data=batch)\n",
    "\n",
    "        # End of epoch handling\n",
    "        if (t+1) % steps_per_epoch == 0:\n",
    "            epoch = (t+1) // steps_per_epoch\n",
    "\n",
    "            # Save model\n",
    "            if (epoch % save_freq == 0) or (epoch == epochs):\n",
    "                pass\n",
    "\n",
    "            # Test the performance of the deterministic version of the agent.\n",
    "            test_agent()\n",
    "\n",
    "            # Log info about epoch\n",
    "            tqdm.write(f\"Epoch {epoch}\")\n",
    "            tqdm.write(f\"Time, {time.time()-start_time}\")\n",
    "    return ac\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    HIDDEN_SIZE = 256\n",
    "    GAMMA = 0.99\n",
    "    SEED = 0\n",
    "    EPOCHS = 100\n",
    "    NUM_LAYERS = 2\n",
    "\n",
    "    policy = ddpg(lambda : gym.make(ENV_NAME), actor_critic=MLPActorCritic,\n",
    "         ac_kwargs=dict(hidden_sizes=[HIDDEN_SIZE]*NUM_LAYERS), \n",
    "         gamma=GAMMA, seed=SEED, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = policy.act(torch.as_tensor(observation, dtype=torch.float32))\n",
    "print(action)\n",
    "observation,*_ = env.step(action)\n",
    "print(observation)\n",
    "render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "\n",
    "\n",
    "# initialize an empty figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# initialize empty lists to store x and y values\n",
    "x_vals = []\n",
    "loss_a_vals = []\n",
    "loss_b_vals = []\n",
    "\n",
    "# plot line for loss A, with empty marker to show only the line\n",
    "loss_a_line, = ax.plot(x_vals, loss_a_vals, '-o', label='Loss A', color='blue', markerfacecolor='none')\n",
    "\n",
    "# plot line for loss B, with empty marker to show only the line\n",
    "loss_b_line, = ax.plot(x_vals, loss_b_vals, '-o', label='Loss B', color='red', markerfacecolor='none')\n",
    "\n",
    "# add legend to the plot\n",
    "ax.legend()\n",
    "\n",
    "# set x and y axis labels\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss value')\n",
    "\n",
    "# set the title of the plot\n",
    "ax.set_title('Loss A and B over epochs')\n",
    "\n",
    "# function to update the plot with new values\n",
    "def update_plot(epoch, loss_a, loss_b):\n",
    "    # append new values to the lists\n",
    "    x_vals.append(epoch)\n",
    "    loss_a_vals.append(loss_a)\n",
    "    loss_b_vals.append(loss_b)\n",
    "    \n",
    "    # update data for the two lines\n",
    "    loss_a_line.set_data(x_vals, loss_a_vals)\n",
    "    loss_b_line.set_data(x_vals, loss_b_vals)\n",
    "    \n",
    "    # set new x and y limits for the plot\n",
    "    ax.set_xlim([min(x_vals) - 1, max(x_vals) + 1])\n",
    "    ax.set_ylim([min(loss_a_vals + loss_b_vals) - 0.1, max(loss_a_vals + loss_b_vals) + 0.1])\n",
    "    \n",
    "    # redraw the plot with the updated data\n",
    "    fig.canvas.draw()\n",
    "\n",
    "# example usage of the update_plot function\n",
    "update_plot(1, 0.5, 0.6)\n",
    "update_plot(2, 0.4, 0.5)\n",
    "update_plot(3, 0.3, 0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Define the PyTorch model\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return Categorical(logits=x)\n",
    "        \n",
    "# Define the training function\n",
    "def train(env, policy, optimizer, num_steps):\n",
    "    rewards = []\n",
    "    obs = env.reset()\n",
    "    for i in range(num_steps):\n",
    "        action = policy(torch.from_numpy(obs).float())\n",
    "        action = action.sample().numpy()\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Compute the loss and perform backpropagation\n",
    "    returns = np.zeros_like(rewards)\n",
    "    running_return = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        running_return = rewards[i] + running_return * 0.99\n",
    "        returns[i] = running_return\n",
    "    returns = torch.tensor(returns)\n",
    "    log_probs = policy(torch.from_numpy(obs).float()).log_prob(torch.from_numpy(action))\n",
    "    loss = -(log_probs * returns).mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return np.sum(rewards)\n",
    "\n",
    "# Define the objective function to optimize with Optuna\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1)\n",
    "    num_steps = trial.suggest_int(\"num_steps\", 100, 1000)\n",
    "    env = gym.make('CartPole-v1')\n",
    "    policy = Policy()\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "    \n",
    "    rewards = []\n",
    "    for i in range(10):\n",
    "        total_reward = train(env, policy, optimizer, num_steps)\n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "\n",
    "    return np.mean(rewards)\n",
    "\n",
    "# Create the Optuna study and run the optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best hyperparameters found by Optuna\n",
    "print(\"Best hyperparameters found: \", study.best_params)\n",
    "print(\"Best reward found: \", study.best_value)\n",
    "print(\"Best trial: \", study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.matplotlib.plot_optimization_history(study)\n",
    "optuna.visualization.matplotlib.plot_slice(study)\n",
    "optuna.visualization.matplotlib.plot_parallel_coordinate(study)\n",
    "optuna.visualization.matplotlib.plot_contour(study)\n",
    "optuna.visualization.matplotlib.plot_intermediate_values(study)\n",
    "optuna.visualization.matplotlib.plot_edf(study)\n",
    "optuna.visualization.matplotlib.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N = 10\n",
    "NUM_SAMPLE = 100\n",
    "CONSTANT_NOISE = torch.rand(NUM_SAMPLE, 10)\n",
    "\n",
    "\n",
    "def _get_noised_obs(self, obs, actions, eps):\n",
    "    M, N, A = obs.shape[0], obs.shape[1], actions.shape[1]\n",
    "    size = 100\n",
    "    delta_s = 2 * eps * self.obs_std * (CONSTANT_NOISE - 0.5) \n",
    "    tmp_obs = obs.reshape(-1, 1, N).repeat(1, size, 1).reshape(-1, N)\n",
    "    delta_s = delta_s.reshape(1, size, N).repeat(M, 1, 1).reshape(-1, N)\n",
    "    noised_obs = tmp_obs + delta_s\n",
    "    return M, A, size, noised_obs, delta_s\n",
    "\n",
    "\n",
    "def _get_noised_obs_plus(self, obs, actions, eps):\n",
    "    M, N, A = obs.shape[0], obs.shape[1], actions.shape[1]\n",
    "    size = self.num_samples\n",
    "    delta_s = 2 * eps * self.obs_std * (CONSTANT_NOISE - 0.5) \n",
    "    tmp_obs = obs.reshape(-1, 1, N).repeat(1, size, 1).reshape(-1, N)\n",
    "    delta_s = delta_s.reshape(1, size, N).repeat(M, 1, 1).reshape(-1, N)\n",
    "    noised_obs = tmp_obs + delta_s\n",
    "    return M, A, size, noised_obs, delta_s\n",
    "\n",
    "\n",
    "\n",
    "# Test the function with example inputs\n",
    "obs = torch.randn(3, 7)  # Example observation tensor\n",
    "actions = torch.randn(3, 2)  # Example actions tensor\n",
    "eps = 0.1  # Example epsilon value\n",
    "result1 = _get_noised_obs(obs, actions, eps)\n",
    "result2 = _get_noised_obs_plus(obs, actions, eps)\n",
    "\n",
    "print (torch.mean((result1[3]-result2[3])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n",
      "tensor([[[ 1.,  2.,  3.],\n",
      "         [ 1.,  2.,  3.]],\n",
      "\n",
      "        [[ 4.,  5.,  6.],\n",
      "         [ 4.,  5.,  6.]],\n",
      "\n",
      "        [[ 7.,  8.,  9.],\n",
      "         [ 7.,  8.,  9.]],\n",
      "\n",
      "        [[10., 11., 12.],\n",
      "         [10., 11., 12.]]])\n",
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.],\n",
      "        [10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "num_qs = 10\n",
    "B = 3\n",
    "\n",
    "q_target = torch.randn(3, 1).requires_grad_()\n",
    "q_target_detach1 = q_target.detach().unsqueeze(0).repeat(num_qs, 1, 1) \n",
    "\n",
    "\n",
    "\n",
    "q_target_detach = q_target.unsqueeze(0).detach()\n",
    "# Use `expand` instead of `repeat` to avoid copying data\n",
    "q_target_detach = q_target_detach.expand(num_qs, -1, -1)\n",
    "\n",
    "\n",
    "# print(q_target_detach1-q_target_detach)\n",
    "# print(q_target_detach)\n",
    "\n",
    "obs = torch.tensor([[1,2,3],[4,5,6],[7,8,9],[10,11,12]],dtype=torch.float)\n",
    "print(obs)\n",
    "print(obs.unsqueeze(1).expand(-1, 2, -1).contiguous())\n",
    "obs1 = obs.repeat_interleave(2, dim=0)\n",
    "print(obs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACTrainerRank1(TorchTrainer):\n",
    "    \"\"\"\n",
    "    Soft Actor Critic (Haarnoja et al. 2018). (Offline training ver.)\n",
    "    Continuous maximum Q-learning algorithm with parameterized actor.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            env,  # Associated environment for learning\n",
    "            policy,  # Associated policy (should be TanhGaussian)\n",
    "            qfs,  # Q functions\n",
    "            target_qfs,  # Slow updater to Q functions\n",
    "            discount=0.99,  # Discount factor\n",
    "            reward_scale=1.0,  # Scaling of rewards to modulate entropy bonus\n",
    "            use_automatic_entropy_tuning=True,  # Whether to use the entropy-constrained variant\n",
    "            target_entropy=None,  # Target entropy for entropy-constraint variant\n",
    "            policy_lr=3e-4,  # Learning rate of policy and entropy weight\n",
    "            qf_lr=3e-4,  # Learning rate of Q functions\n",
    "            optimizer_class=optim.Adam,  # Class of optimizer for all networks\n",
    "            soft_target_tau=5e-3,  # Rate of update of target networks\n",
    "            target_update_period=1,  # How often to update target networks\n",
    "            max_q_backup=False,\n",
    "            deterministic_backup=False,\n",
    "            policy_eval_start=0,\n",
    "            eta=-1.0,\n",
    "            num_qs=10,\n",
    "            replay_buffer=None,\n",
    "            imagination=None,\n",
    "\n",
    "            # smoothing\n",
    "            num_samples=20,\n",
    "            policy_smooth_eps=0,\n",
    "            policy_smooth_reg=0.0,\n",
    "            q_smooth_eps=0,\n",
    "            q_smooth_reg=0.0,\n",
    "            q_smooth_tau=0.2,\n",
    "            norm_input=False,\n",
    "            obs_std=1,\n",
    "            q_ood_eps=0,\n",
    "            q_ood_reg = 0,\n",
    "            q_ood_uncertainty_reg=0,\n",
    "            q_ood_uncertainty_reg_min=0,\n",
    "            q_ood_uncertainty_decay=1e-6,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.qfs = qfs\n",
    "        self.target_qfs = target_qfs\n",
    "        self.target_qfs.eval()\n",
    "        for param in target_qfs.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.num_qs = num_qs\n",
    "\n",
    "        self.discount = discount\n",
    "        self.reward_scale = reward_scale\n",
    "        self.soft_target_tau = soft_target_tau\n",
    "        self.target_update_period = target_update_period\n",
    "\n",
    "        self.max_q_backup = max_q_backup\n",
    "        self.deterministic_backup = deterministic_backup\n",
    "        self.eta = eta\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "        #### robust \n",
    "        self.num_samples = num_samples\n",
    "        self.policy_smooth_eps = policy_smooth_eps\n",
    "        self.policy_smooth_reg = policy_smooth_reg\n",
    "        self.q_smooth_eps = q_smooth_eps\n",
    "        self.q_smooth_reg = q_smooth_reg\n",
    "        self.q_smooth_tau = q_smooth_tau\n",
    "        self.obs_std = 1 if not norm_input else ptu.from_numpy(obs_std)\n",
    "        self.q_ood_eps = q_ood_eps\n",
    "        self.q_ood_reg = q_ood_reg\n",
    "        self.q_ood_uncertainty_reg = q_ood_uncertainty_reg\n",
    "        self.q_ood_uncertainty_reg_min = q_ood_uncertainty_reg_min\n",
    "        self.q_ood_uncertainty_decay = q_ood_uncertainty_decay\n",
    "\n",
    "        self.use_automatic_entropy_tuning = use_automatic_entropy_tuning\n",
    "        if self.use_automatic_entropy_tuning:\n",
    "            if target_entropy:\n",
    "                self.target_entropy = target_entropy\n",
    "            else:\n",
    "                # Heuristic value: dimension of action space\n",
    "                self.target_entropy = -np.prod(\n",
    "                    self.env.action_space.shape).item()\n",
    "            self.log_alpha = ptu.zeros(1, requires_grad=True)\n",
    "            self.alpha_optimizer = optimizer_class(\n",
    "                [self.log_alpha],\n",
    "                lr=policy_lr,\n",
    "            )\n",
    "\n",
    "        self.qf_criterion = nn.MSELoss(reduction='none')\n",
    "\n",
    "        self.policy_optimizer = optimizer_class(\n",
    "            self.policy.parameters(),\n",
    "            lr=policy_lr,\n",
    "        )\n",
    "        # Add weight decay\n",
    "        self.qfs_optimizer = optimizer_class(\n",
    "            self.qfs.parameters(),\n",
    "            lr=qf_lr,\n",
    "            weight_decay=1e-4,\n",
    "        ) \n",
    "\n",
    "        self.eval_statistics = OrderedDict()\n",
    "        self._need_to_update_eval_statistics = True\n",
    "        self.policy_eval_start = policy_eval_start\n",
    "\n",
    "    def _get_tensor_values(self, obs, actions, network=None):\n",
    "        action_shape = actions.shape[0]\n",
    "        B,E = obs.shape\n",
    "        num_repeat = action_shape // B\n",
    "        preds = network(obs.unsqueeze(1).expand(-1,num_repeat,-1).contiguous().view(-1,E), actions)\n",
    "        # preds size (ensembles, B*Num_repeat,E) => (ensembles, B, Num_repeat, E)\n",
    "        preds = preds.view(-1, B, num_repeat, 1)\n",
    "        return preds\n",
    "\n",
    "    def _get_policy_actions(self, obs, num_actions, network=None):\n",
    "        new_obs_actions, _, _, new_obs_log_pi, *_ = network(\n",
    "            obs.unsqueeze(1).expand(-1, num_actions,-1).contiguous().view(-1,obs.shape[1]),\n",
    "            reparameterize=True,\n",
    "            return_log_prob=True,\n",
    "        )\n",
    "        return new_obs_actions.detach(), new_obs_log_pi.view(\n",
    "            obs.shape[0], num_actions, 1).detach()\n",
    "        \n",
    "\n",
    "    def _get_noised_obs(self, obs, actions, eps):\n",
    "        \"\"\" Add noise to observations \n",
    "            Each observation is repeated interleave num_samples times\n",
    "        \"\"\"\n",
    "        M, N = obs.shape[0], obs.shape[1] \n",
    "        A = actions.shape[1]\n",
    "        size = self.num_samples\n",
    "        delta_s = 2 * eps * self.obs_std * (torch.rand(size, N, device=ptu.device) - 0.5) \n",
    "        tmp_obs = obs.unsqueeze(1).expand(-1, size, -1).contiguous()  \n",
    "        noised_obs = tmp_obs + delta_s\n",
    "        return M, A, size, noised_obs.view(-1,N)\n",
    "\n",
    "\n",
    "    def train_from_torch(self, batch, indices):\n",
    "        obs= batch['observations']\n",
    "        next_obs = batch['next_observations']\n",
    "        actions = batch['actions']\n",
    "        rewards = batch['rewards']\n",
    "        terminals = batch['terminals']\n",
    "        \n",
    "        if self.eta > 0:\n",
    "            actions.requires_grad_(True)\n",
    "        \n",
    "        \"\"\"\n",
    "        Policy and Alpha Loss\n",
    "        \"\"\"\n",
    "        # (B,A)= (256,6)             \n",
    "        new_obs_actions, policy_mean, policy_log_std, log_pi, *_ = self.policy(\n",
    "            obs,\n",
    "            reparameterize=True,\n",
    "            return_log_prob=True,\n",
    "        )\n",
    "\n",
    "        if self.use_automatic_entropy_tuning:\n",
    "            alpha_loss = -(self.log_alpha *\n",
    "                           (log_pi + self.target_entropy).detach()).mean()\n",
    "            alpha = self.log_alpha.exp()\n",
    "        else:\n",
    "            alpha_loss = 0\n",
    "            alpha = 1\n",
    "        # (B,1)\n",
    "        q_new_actions = self.qfs.sample(obs, new_obs_actions)  # TODO: Consider to detach Q value\n",
    "\n",
    "        policy_loss = (alpha * log_pi - q_new_actions).mean()\n",
    "\n",
    "        if self._num_train_steps < self.policy_eval_start:\n",
    "            \"\"\"\n",
    "            For the initial few epochs, try doing behaivoral cloning, if needed\n",
    "            conventionally, there's not much difference in performance with having 20k \n",
    "            gradient steps here, or not having it\n",
    "            \"\"\"\n",
    "            policy_log_prob = self.policy.get_log_probs(obs.detach(), actions)\n",
    "            policy_loss = (alpha * log_pi - policy_log_prob).mean()\n",
    "\n",
    "        \n",
    "        if self.policy_smooth_eps > 0 and self.policy_smooth_reg > 0:\n",
    "            M, A, size, noised_obs = self._get_noised_obs(obs, actions, self.policy_smooth_eps)\n",
    "            # (B*size,A)\n",
    "            _, noised_policy_mean, noised_policy_log_std, _, *_ = self.policy(noised_obs,reparameterize=True)\n",
    "            # action_dist = torch.distributions.Normal(policy_mean.repeat_interleave(size, dim=0),\n",
    "            #                                         policy_log_std.exp().repeat_interleave(size, dim=0))\n",
    "            action_dist = torch.distributions.Normal(policy_mean.unsqueeze(1).expand(-1,size,-1).contiguous().view(-1,A),\n",
    "                                                    policy_log_std.exp().unsqueeze(1).expand(-1,size,-1).contiguous().view(-1,A))                                      \n",
    "            noised_action_dist = torch.distributions.Normal(noised_policy_mean, noised_policy_log_std.exp())\n",
    "            kl_loss = kl_divergence(action_dist, noised_action_dist).sum(axis=-1) + kl_divergence(noised_action_dist, action_dist).sum(axis=-1)\n",
    "            kl_loss = kl_loss.view(M, size)\n",
    "            max_values, _ = torch.max(kl_loss, dim=1)\n",
    "            kl_loss_max = torch.mean(max_values)\n",
    "\n",
    "\n",
    "\n",
    "            # noised_states_selected = noised_obs[np.arange(M), max_id]\n",
    "            policy_loss += self.policy_smooth_reg * kl_loss_max\n",
    "            if self._need_to_update_eval_statistics:\n",
    "                self.eval_statistics['Policy Smooth Loss'] = ptu.get_numpy(kl_loss_max) * self.policy_smooth_reg\n",
    "\n",
    "        \"\"\"\n",
    "        QF Loss\n",
    "        \"\"\"\n",
    "        # (num_qs, batch_size, output_size) (M,B,1)\n",
    "        qs_pred = self.qfs(obs, actions)\n",
    "\n",
    "        new_next_actions, _, _, new_log_pi, *_ = self.policy(\n",
    "            next_obs,\n",
    "            reparameterize=False,\n",
    "            return_log_prob=True,\n",
    "        )\n",
    "\n",
    "        if not self.max_q_backup:\n",
    "            with torch.no_grad():\n",
    "                target_q_values = self.target_qfs.sample(next_obs, new_next_actions)\n",
    "                if not self.deterministic_backup:\n",
    "                    target_q_values -= alpha * new_log_pi\n",
    "        else:\n",
    "            # if self.max_q_backup\n",
    "            next_actions_temp, _ = self._get_policy_actions(\n",
    "                next_obs, num_actions=10, network=self.policy)\n",
    "            target_q_values = self._get_tensor_values(\n",
    "                next_obs, next_actions_temp,\n",
    "                network=self.qfs).max(2)[0].min(0)[0]\n",
    "\n",
    "        future_values = (1. - terminals) * self.discount * target_q_values\n",
    "        q_target = self.reward_scale * rewards + future_values\n",
    "\n",
    "        q_target_detach = q_target.unsqueeze(0)\n",
    "        # Use `expand` instead of `repeat` to avoid copying data\n",
    "        q_target_detach = q_target_detach.expand(self.num_qs, -1, -1).contiguous()\n",
    "        qfs_loss = self.qf_criterion(qs_pred, q_target_detach)\n",
    "        qfs_loss = qfs_loss.mean(dim=(1, 2)).sum()\n",
    "        qfs_loss_total = qfs_loss\n",
    "        if self.q_smooth_eps > 0 and self.q_smooth_reg > 0:\n",
    "            M, A, size, noised_obs = self._get_noised_obs(obs, actions, self.q_smooth_eps)\n",
    "            # (M,B*size,1)\n",
    "            noised_qs_pred = self.qfs(noised_obs, actions.unsqueeze(1).expand(-1,size,-1).contiguous().view(-1,A))\n",
    "            diff = noised_qs_pred - qs_pred.expand(-1, -1, size).contiguous().view(self.num_qs, -1, 1)     \n",
    "            pos = torch.clamp(diff, min=0)\n",
    "            neg = torch.clamp(diff, max=0)\n",
    "            noise_Q_loss = (1-self.q_smooth_tau) *  pos.square().mean(axis=0) + self.q_smooth_tau * neg.square().mean(axis=0)\n",
    "            noise_Q_loss = noise_Q_loss.view(M, size)\n",
    "            noise_Q_loss_max = torch.mean(torch.max(noise_Q_loss,dim=1)[0])  \n",
    "            qfs_loss_total += self.q_smooth_reg * noise_Q_loss_max\n",
    "            if self._need_to_update_eval_statistics:\n",
    "                self.eval_statistics['Q Smooth Loss'] = ptu.get_numpy(noise_Q_loss_max) * self.q_smooth_reg\n",
    "                \n",
    "        if self.q_ood_reg > 0: # self.q_ood_eps = 0 for PBRL\n",
    "            ood_loss = torch.zeros(1, device=ptu.device)[0]\n",
    "            if self.q_ood_uncertainty_reg > 0:\n",
    "                M, A, size, noised_obs = self._get_noised_obs(obs, actions, self.q_ood_eps)\n",
    "                ood_actions, _, _, _, *_ = self.policy(noised_obs, reparameterize=False)\n",
    "                ood_qs_pred = self.qfs(noised_obs, ood_actions)\n",
    "                ood_target = ood_qs_pred - self.q_ood_uncertainty_reg * ood_qs_pred.std(axis=0)\n",
    "                ood_loss = self.qf_criterion(ood_target.detach(), ood_qs_pred).mean()\n",
    "                qfs_loss_total += self.q_ood_reg * ood_loss\n",
    "\n",
    "            if self.q_ood_uncertainty_reg > 0:\n",
    "                self.q_ood_uncertainty_reg = max(self.q_ood_uncertainty_reg - self.q_ood_uncertainty_decay, self.q_ood_uncertainty_reg_min)\n",
    "            if self._need_to_update_eval_statistics:\n",
    "                self.eval_statistics['Q OOD Loss'] = ptu.get_numpy(ood_loss) * self.q_ood_reg\n",
    "                self.eval_statistics['q_ood_uncertainty_reg'] = self.q_ood_uncertainty_reg\n",
    "        \n",
    "        if self.eta > 0:\n",
    "            qs_pred_grads = None\n",
    "            sample_size = min(qs_pred.size(0), actions.size(1))\n",
    "            indices = np.random.choice(qs_pred.size(0), size=sample_size, replace=False)\n",
    "            indices = torch.from_numpy(indices).long().to(ptu.device)\n",
    "\n",
    "            obs_tile = obs.unsqueeze(0).expand(self.num_qs, -1, -1).contiguous()\n",
    "            actions_tile = actions.unsqueeze(0).expand(self.num_qs, -1, -1).contiguous().requires_grad_(True)\n",
    "            qs_preds_tile = self.qfs(obs_tile, actions_tile)\n",
    "            qs_pred_grads, = torch.autograd.grad(qs_preds_tile.sum(), actions_tile, retain_graph=True, create_graph=True)\n",
    "            qs_pred_grads = qs_pred_grads / (torch.norm(qs_pred_grads, p=2, dim=2).unsqueeze(-1) + 1e-10)\n",
    "\n",
    "            qs_pred_grads = torch.index_select(qs_pred_grads, dim=0, index=indices).transpose(0, 1)\n",
    "            \n",
    "            qs_pred_grads = torch.einsum('bik,bjk->bij', qs_pred_grads, qs_pred_grads)\n",
    "            masks = torch.eye(sample_size, device=ptu.device).unsqueeze(dim=0).expand(qs_pred_grads.size(0), -1, -1).contiguous()\n",
    "            qs_pred_grads = (1 - masks) * qs_pred_grads\n",
    "            grad_loss = torch.mean(torch.sum(qs_pred_grads, dim=(1, 2))) / (sample_size - 1)\n",
    "            \n",
    "            qfs_loss_total += self.eta * grad_loss\n",
    "\n",
    "        if self.use_automatic_entropy_tuning and not self.deterministic_backup:\n",
    "            self.alpha_optimizer.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optimizer.step()\n",
    "\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "        self.qfs_optimizer.zero_grad()\n",
    "        qfs_loss_total.backward()\n",
    "        self.qfs_optimizer.step()\n",
    "\n",
    "        self.try_update_target_networks()\n",
    "        \"\"\"\n",
    "        Save some statistics for eval\n",
    "        \"\"\"\n",
    "        if self._need_to_update_eval_statistics:\n",
    "            self._need_to_update_eval_statistics = False\n",
    "\n",
    "            policy_loss = ptu.get_numpy(log_pi - q_new_actions).mean()\n",
    "            policy_avg_std = ptu.get_numpy(torch.exp(policy_log_std)).mean()\n",
    "            self.eval_statistics['QFs Loss'] = np.mean(\n",
    "                ptu.get_numpy(qfs_loss)) / self.num_qs\n",
    "            if self.eta > 0:\n",
    "                self.eval_statistics['Q Grad Loss'] = np.mean(\n",
    "                    ptu.get_numpy(grad_loss))\n",
    "            self.eval_statistics['Policy Loss'] = np.mean(policy_loss)\n",
    "\n",
    "            self.eval_statistics.update(\n",
    "                create_stats_ordered_dict(\n",
    "                    'Qs Predictions',\n",
    "                    ptu.get_numpy(qs_pred),\n",
    "                ))\n",
    "            self.eval_statistics.update(\n",
    "                create_stats_ordered_dict(\n",
    "                    'Qs Targets',\n",
    "                    ptu.get_numpy(q_target),\n",
    "                ))\n",
    "\n",
    "            self.eval_statistics.update(\n",
    "                create_stats_ordered_dict(\n",
    "                    'Log Pis',\n",
    "                    ptu.get_numpy(log_pi),\n",
    "                ))\n",
    "            self.eval_statistics.update(\n",
    "                create_stats_ordered_dict(\n",
    "                    'Policy mu',\n",
    "                    ptu.get_numpy(policy_mean),\n",
    "                ))\n",
    "            self.eval_statistics.update(\n",
    "                create_stats_ordered_dict(\n",
    "                    'Policy log std',\n",
    "                    ptu.get_numpy(policy_log_std),\n",
    "                ))\n",
    "            self.eval_statistics['Policy std'] = np.mean(policy_avg_std)\n",
    "\n",
    "            if self.use_automatic_entropy_tuning:\n",
    "                self.eval_statistics['Alpha'] = alpha.item()\n",
    "                self.eval_statistics['Alpha Loss'] = alpha_loss.item()\n",
    "\n",
    "    def try_update_target_networks(self):\n",
    "        if self._num_train_steps % self.target_update_period == 0:\n",
    "            self.update_target_networks()\n",
    "\n",
    "    def update_target_networks(self):\n",
    "        ptu.soft_update_from_to(self.qfs, self.target_qfs,\n",
    "                                self.soft_target_tau)\n",
    "\n",
    "    def get_diagnostics(self):\n",
    "        return self.eval_statistics\n",
    "\n",
    "    def end_epoch(self, epoch):\n",
    "        self._need_to_update_eval_statistics = True\n",
    "\n",
    "    @property\n",
    "    def networks(self):\n",
    "        base_list = [\n",
    "            self.policy,\n",
    "            self.qfs,\n",
    "            self.target_qfs,\n",
    "        ]\n",
    "\n",
    "        return base_list\n",
    "\n",
    "    def get_snapshot(self):\n",
    "        return dict(\n",
    "            policy=self.policy,\n",
    "            qfs=self.qfs,\n",
    "            target_qfs=self.qfs,\n",
    "            log_alpha=self.log_alpha,\n",
    "            policy_optim=self.policy_optimizer,\n",
    "            qfs_optim=self.qfs_optimizer,\n",
    "            alpha_optim=self.alpha_optimizer,\n",
    "        )\n",
    "    \n",
    "    def load_snapshot(self, path):\n",
    "        datas = torch.load(path, map_location=ptu.device)\n",
    "        self.policy.load_state_dict(datas['trainer/policy'].state_dict())\n",
    "        self.qfs.load_state_dict(datas['trainer/qfs'].state_dict())\n",
    "        self.target_qfs.load_state_dict(datas['trainer/target_qfs'].state_dict())\n",
    "        self.log_alpha = datas['trainer/log_alpha']\n",
    "        self.policy_optimizer.load_state_dict( datas['trainer/policy_optim'].state_dict())\n",
    "        self.qfs_optimizer.load_state_dict(datas['trainer/qfs_optim'].state_dict())\n",
    "        self.alpha_optimizer.load_state_dict(datas['trainer/alpha_optim'].state_dict())\n",
    "        logger.log('Loading model from {} finished'.format(path))\n",
    "    \n",
    "    def load_qfs(self, path):\n",
    "        datas = torch.load(path, map_location=ptu.device)\n",
    "        self.qfs.load_state_dict(datas['trainer/qfs'].state_dict())\n",
    "        logger.log('Loading Q networks from {} finished'.format(path))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def simplify_dict(d):\n",
    "    \"\"\"\n",
    "    Recursive helper function to simplify a nested dictionary.\n",
    "\n",
    "    Args:\n",
    "        d (dict): The input dictionary.\n",
    "\n",
    "    Returns:\n",
    "        dict: The simplified dictionary without any inner dictionaries.\n",
    "    \"\"\"\n",
    "    return {k: v if not isinstance(v, dict) else simplify_dict(v) for k, v in d.items()}\n",
    "\n",
    "\n",
    "\n",
    "json_str = {\n",
    "  \"algorithm\": \"\",\n",
    "  \"collector_type\": \"step\",\n",
    "  \"env_kwargs\": {},\n",
    "  \"env_name\": \"hopper-medium-expert-v2\",\n",
    "  \"eval_attack\": False,\n",
    "  \"eval_attack_eps\": 0.01,\n",
    "  \"eval_attack_mode\": \"random\",\n",
    "  \"eval_no_training\": False,\n",
    "  \"load_Qs\": \"\",\n",
    "  \"load_path\": \"\",\n",
    "  \"norm_input\": True,\n",
    "  \"offline_kwargs\": {\n",
    "    \"batch_size\": 512,\n",
    "    \"max_path_length\": 1000,\n",
    "    \"num_epochs\": 500,\n",
    "    \"num_eval_steps_per_epoch\": 1000,\n",
    "    \"num_trains_per_train_loop\": 1000,\n",
    "    \"save_snapshot_freq\": 500\n",
    "  },\n",
    "  \"policy_kwargs\": {\n",
    "    \"layer_size\": 256,\n",
    "    \"num_p_layers\": 3,\n",
    "    \"num_q_layers\": 3\n",
    "  },\n",
    "  \"replay_buffer_size\": 2000000,\n",
    "  \"reward_mean\": False,\n",
    "  \"reward_std\": False,\n",
    "  \"seed\": 0,\n",
    "  \"trainer_kwargs\": {\n",
    "    \"deterministic_backup\": False,\n",
    "    \"discount\": 0.99,\n",
    "    \"eta\": -1.0,\n",
    "    \"max_q_backup\": False,\n",
    "    \"num_qs\": 29,\n",
    "    \"num_samples\": 20,\n",
    "    \"policy_eval_start\": 0,\n",
    "    \"policy_lr\": 0.007444261208897999,\n",
    "    \"policy_smooth_eps\": 0.005,\n",
    "    \"policy_smooth_reg\": 0.1,\n",
    "    \"q_ood_eps\": 0.01,\n",
    "    \"q_ood_reg\": 0.5,\n",
    "    \"q_ood_uncertainty_decay\": 1e-06,\n",
    "    \"q_ood_uncertainty_reg\": 3.0,\n",
    "    \"q_ood_uncertainty_reg_min\": 1.0,\n",
    "    \"q_smooth_eps\": 0.005,\n",
    "    \"q_smooth_reg\": 0.0001,\n",
    "    \"q_smooth_tau\": 0.2,\n",
    "    \"qf_lr\": 0.0017850021952371426,\n",
    "    \"soft_target_tau\": 0.005,\n",
    "    \"target_update_period\": 1,\n",
    "    \"use_automatic_entropy_tuning\": True\n",
    "  }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data from wandb and push back to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "api = wandb.Api()\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def get_data_from_run(name,entity=\"aimrl\",project=\"MASTER_UNCERTAINTY\"):\n",
    "    '''Get the data from a wandb run. Data is only a sample of 500 data from the run.\n",
    "    \n",
    "    Returns: dataframe and config\n",
    "    '''\n",
    "    # name = 3pe2wl12\"\n",
    "    # run is specified by <entity>/<project>/<run_id>\n",
    "    run = api.run(f\"{entity}/{project}/{name}\")\n",
    "\n",
    "    # save the metrics for the run to a csv file\n",
    "    metrics_dataframe = run.history()\n",
    "    config = run.config\n",
    "    run.wait_until_finished()\n",
    "\n",
    "\n",
    "    headers = metrics_dataframe.columns.values.tolist()\n",
    "    for key in headers:\n",
    "        proc_key = key\n",
    "        proc_key = proc_key.replace(' (s)', '')\n",
    "        proc_key = proc_key.replace(' ', '_')\n",
    "        proc_key = proc_key.lower()\n",
    "        if '/' not in key or 'replay_buffer' in key:\n",
    "            proc_key = 'misc/' + proc_key\n",
    "        \n",
    "        metrics_dataframe.rename(columns={key: proc_key}, inplace=True)\n",
    "\n",
    "    \n",
    "    headers = metrics_dataframe.columns.values.tolist()\n",
    "    print(headers)\n",
    "\n",
    "    # Loop through columns and convert values to float\n",
    "    for col in metrics_dataframe:\n",
    "        if col not in metrics_dataframe:  # Skip column names\n",
    "            continue\n",
    "        metrics_dataframe[col] = pd.to_numeric(metrics_dataframe[col], errors='coerce')\n",
    "    return metrics_dataframe, config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_run_full(name,entity=\"aimrl\",project=\"MASTER_UNCERTAINTY\"):\n",
    "    '''Return a list of dictionaries data for each step, and the config'''\n",
    "    # name = 3pe2wl12\"\n",
    "    run = api.run(f\"{entity}/{project}/{name}\")\n",
    "\n",
    "    data_dict_iterator = run.scan_history()\n",
    "    config = run.config\n",
    "    data_list = []\n",
    "    for data_dict in data_dict_iterator:\n",
    "        log_dict = {}\n",
    "        for key in data_dict.keys():\n",
    "            proc_key = key\n",
    "            proc_key = proc_key.replace(' (s)', '')\n",
    "            proc_key = proc_key.replace(' ', '_')\n",
    "            proc_key = proc_key.lower()\n",
    "            if '/' not in key or 'replay_buffer' in key:\n",
    "                proc_key = 'misc/' + proc_key\n",
    "            log_dict[proc_key] = float(data_dict[key])    \n",
    "            \n",
    "        data_list.append(log_dict)\n",
    "    \n",
    "    return data_list, config\n",
    "\n",
    "\n",
    "def push_data_to_wandb(datalist,group=\"0\",name=\"test\",entity=\"aimrl\",project=\"MASTER_UNCERTAINTY\" , config=None):\n",
    "    '''Push data to wandb. Data is a list of dictionaries. Each dictionary is a step.\n",
    "    '''\n",
    "    run = wandb.init(project=project, entity=entity, name=name, group=group,config=config)\n",
    "    for i in range(len(datalist)):\n",
    "        wandb.log(datalist[i])\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 20:50:29.460086: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-17 20:50:29.679793: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/mnt/hdd/thanh/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-04-17 20:50:29.679832: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-17 20:50:30.668113: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/mnt/hdd/thanh/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-04-17 20:50:30.668231: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/mnt/hdd/thanh/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-04-17 20:50:30.668244: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/hdd/thanh/workspace/2023/common/RORL/wandb/run-20230417_205027-9ft98zhu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/aimrl/MASTER_UNCERTAINTY/runs/9ft98zhu\" target=\"_blank\">walker2d-medium-replay-v2_RORL_BASELINE_SEED0</a></strong> to <a href=\"https://wandb.ai/aimrl/MASTER_UNCERTAINTY\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f39978fddf4dd981b8a64180d7765b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>evaluation/actions_mean</td><td>▁▃█▂▂▃▃▃▃▇▃▃▂▇▂▂▃▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂</td></tr><tr><td>evaluation/actions_std</td><td>▃▇▁▇██▇▇▇▂█▇█▃██████▇▇█▇█▇▇▇▇▇█▇▇▇▇▇▇███</td></tr><tr><td>evaluation/average_returns</td><td>▁▂▁▇▇▇▇▇▇▁▇▇▇▁▇▇▇▇▇█▇███████▇███████████</td></tr><tr><td>evaluation/num_paths</td><td>▆▆█▁▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>evaluation/num_paths_total</td><td>▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>evaluation/num_steps_total</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>evaluation/path_length_mean</td><td>▂▂▁██████▁███▁██████████████████████████</td></tr><tr><td>evaluation/path_length_std</td><td>█▄▅▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>evaluation/returns_mean</td><td>▁▂▁▇▇▇▇▇▇▁▇▇▇▁▇▇▇▇▇█▇███████▇███████████</td></tr><tr><td>evaluation/returns_std</td><td>█▅▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>evaluation/rewards_mean</td><td>▂▅▁▇▇▇▇▇▇▁▇▇▇▁▇▇▇▇▇▇▇██████▇▇█▇▇███████▇</td></tr><tr><td>evaluation/rewards_std</td><td>▆█▁▆▆▆▆▇█▁▆█▆▁▅▅▇▅▅▆▇▇▆▇▆▆▇▇▇▇▆▇▆▆▆▆▆▅▄▅</td></tr><tr><td>misc/_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>misc/_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>misc/_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>misc/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>misc/replay_buffer/size</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>policy_trainer/alpha</td><td>█▂▂▁▂▂▂▁▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>policy_trainer/alpha_loss</td><td>▁▇▅▆▇██▇▇▇▇▆▇▇▆▆▇▆▇▆▇▇█▆▆▆▇▆▇▆▇▇▇▇▇▇▇▇▆▆</td></tr><tr><td>policy_trainer/log_pis_mean</td><td>▁▇▆▇▇██▇▇▇▇▇▇▇▆▆▇▆█▆▇▇█▇▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>policy_trainer/log_pis_std</td><td>▁▅▅▅▅▇▅▆▆▅▅▇▇▇█▆▅▅▅▇▆▅▇▅▄▅█▅▅▆▇▆▇▃▆▅▅█▆▅</td></tr><tr><td>policy_trainer/policy_log_std_mean</td><td>█▁▃▂▃▃▂▃▂▃▃▃▃▃▂▂▂▂▃▁▂▂▂▂▂▂▂▂▂▂▃▁▂▂▂▂▂▂▂▂</td></tr><tr><td>policy_trainer/policy_log_std_std</td><td>▁█▆▇▇▆▆▆▆▇▇▆▇▇▆▆▆▇▆▇▇▆▇▆▆▆▆▇▆▆▇▆▆▆▆▆▆▇▇▆</td></tr><tr><td>policy_trainer/policy_loss</td><td>█▅▇▅▅▃▃▃▃▄▃▅▄▄▃▂▂▃▃▃▃▂▃▄▁▂▂▂▁▁▃▂▂▁▁▁▃▁▅▃</td></tr><tr><td>policy_trainer/policy_mu_mean</td><td>▁▇▇▆▄██▄▅▄▅▇▅▄▃▄▇▄▅▅▄▇▆▆▆▄▄▇▅▆▄▆▄▅▅▇▆▄▅▄</td></tr><tr><td>policy_trainer/policy_mu_std</td><td>▁▆▆▇█▇▆▇▇▇▇▇██▇▇▇▇▇▇█▆█▇▇▇▇▆▇▇▇▇▇▆▇▆▇█▇▇</td></tr><tr><td>policy_trainer/policy_smooth_loss</td><td>▁▃▄▅▄▅▄▅▄▇▅▅▆▅▅▇▆▅▅▇▇▅▇▇▇▇▇▆▆▇▇▇▆▇▆█▇▆▇▇</td></tr><tr><td>policy_trainer/policy_std</td><td>█▁▃▂▃▃▂▂▂▃▃▃▃▃▂▁▂▂▂▁▂▂▂▂▂▂▂▂▂▂▃▁▂▂▂▁▂▁▂▂</td></tr><tr><td>policy_trainer/q_ood_loss</td><td>▁█▇▅▇▆▅▆▇▆▆█▆█▆▅▇▆█▆▇▆▇▇▅▆▆▅▇▅▆▆▆▅▆▅▇▆▆▆</td></tr><tr><td>policy_trainer/q_ood_uncertainty_reg</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>policy_trainer/q_smooth_loss</td><td>▁▅▄▃▃▄█▅▇▆▅▆▅▆▄▄▅▄▅▅▄▄▅▅▄▅▆▇▄▅▅▃▅█▆▆▆▅▆▅</td></tr><tr><td>policy_trainer/qfs_loss</td><td>▂█▇▃▇▆▃▃▃▃▄▅▄▃▂▂▃▂▃▂▃▃▃▃▄▁▂▂▃▃▃▄▂▂▁▂▂▁▃▃</td></tr><tr><td>policy_trainer/qs_predictions_mean</td><td>▁▄▂▅▄▆▆▆▆▆▆▅▅▆▆▇▇▆▇▆▆▇▆▅█▇█▇██▆▇▇███▆█▅▆</td></tr><tr><td>policy_trainer/qs_predictions_std</td><td>▁▇▅▅▇▇▅▆▆▇▇█▇▇▇▆▇▇▇▇▆▆▆█▆▇▇▇▆▇▇▇▇▆▆▆▇▇▇▇</td></tr><tr><td>policy_trainer/qs_targets_mean</td><td>▁▄▂▅▄▆▆▆▆▅▆▅▅▅▆▇▇▆▆▆▆▇▆▅█▇█▇██▆▇▇▇██▆█▄▆</td></tr><tr><td>policy_trainer/qs_targets_std</td><td>▁▇▅▆▆▆▅▆▆▇▇█▇▇▇▆▇▇▇▇▆▆▆█▆▇▇▇▆▇▇▇▇▆▆▆▇▇▇▇</td></tr><tr><td>time/epoch</td><td>▇▇▃▄▃▅▅▁▂▅▅▄▅▄▃▆▆▇▇███▇▇▇▇▆▇█▇▇▇▇▆█▆▇▆▇▆</td></tr><tr><td>time/evaluation_sampling</td><td>▃▆█▂▁▁▂▂▁▆▃▂▂▂▄▅▂▂▂▂▃▂▃▄▁▄▆▁▃▄▂▂▂▄▂▄▂▂▅▂</td></tr><tr><td>time/logging</td><td>▆▂▄▃▁▆▃▂▁▁▂▆█▁▅▅▃▅▄▃▂▂▂▃▂▃▂▄▂▅▅▃▂▂▂▂▄▂▅▂</td></tr><tr><td>time/saving</td><td>▆▅▂▃▁▅▅▅▁▃▄▅█▅▅▇▅▆▅▃▃▃▄▄▃▅▄▄▄▅▆▄▅▃▄▄▆▃▆▃</td></tr><tr><td>time/total</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/training</td><td>▇▆▂▄▃▅▅▁▂▄▅▄▅▄▃▅▆▇▆▇██▇▇▇▇▆▇▇▇▇▇▇▆█▆▇▆▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>evaluation/actions_mean</td><td>0.41876</td></tr><tr><td>evaluation/actions_std</td><td>0.58835</td></tr><tr><td>evaluation/average_returns</td><td>3690.06492</td></tr><tr><td>evaluation/num_paths</td><td>1.0</td></tr><tr><td>evaluation/num_paths_total</td><td>1188.0</td></tr><tr><td>evaluation/num_steps_total</td><td>966109.0</td></tr><tr><td>evaluation/path_length_mean</td><td>1000.0</td></tr><tr><td>evaluation/path_length_std</td><td>0.0</td></tr><tr><td>evaluation/returns_mean</td><td>3690.06492</td></tr><tr><td>evaluation/returns_std</td><td>0.0</td></tr><tr><td>evaluation/rewards_mean</td><td>3.69006</td></tr><tr><td>evaluation/rewards_std</td><td>0.90353</td></tr><tr><td>misc/_runtime</td><td>37118.0</td></tr><tr><td>misc/_step</td><td>999.0</td></tr><tr><td>misc/_timestamp</td><td>1681658296.0</td></tr><tr><td>misc/epoch</td><td>999.0</td></tr><tr><td>misc/replay_buffer/size</td><td>301698.0</td></tr><tr><td>policy_trainer/alpha</td><td>0.05769</td></tr><tr><td>policy_trainer/alpha_loss</td><td>2.03865</td></tr><tr><td>policy_trainer/log_pis_mean</td><td>6.71464</td></tr><tr><td>policy_trainer/log_pis_std</td><td>3.75557</td></tr><tr><td>policy_trainer/policy_log_std_mean</td><td>-0.99815</td></tr><tr><td>policy_trainer/policy_log_std_std</td><td>0.27779</td></tr><tr><td>policy_trainer/policy_loss</td><td>-108.11125</td></tr><tr><td>policy_trainer/policy_mu_mean</td><td>0.76199</td></tr><tr><td>policy_trainer/policy_mu_std</td><td>1.26048</td></tr><tr><td>policy_trainer/policy_smooth_loss</td><td>0.04661</td></tr><tr><td>policy_trainer/policy_std</td><td>0.38299</td></tr><tr><td>policy_trainer/q_ood_loss</td><td>0.00822</td></tr><tr><td>policy_trainer/q_ood_uncertainty_reg</td><td>0.1</td></tr><tr><td>policy_trainer/q_smooth_loss</td><td>0.00023</td></tr><tr><td>policy_trainer/qfs_loss</td><td>17.91298</td></tr><tr><td>policy_trainer/qs_predictions_mean</td><td>115.53541</td></tr><tr><td>policy_trainer/qs_predictions_std</td><td>66.16812</td></tr><tr><td>policy_trainer/qs_targets_mean</td><td>115.66492</td></tr><tr><td>policy_trainer/qs_targets_std</td><td>66.00568</td></tr><tr><td>time/epoch</td><td>37.40741</td></tr><tr><td>time/evaluation_sampling</td><td>1.31583</td></tr><tr><td>time/logging</td><td>0.00476</td></tr><tr><td>time/saving</td><td>0.01494</td></tr><tr><td>time/total</td><td>37102.76282</td></tr><tr><td>time/training</td><td>36.07189</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">walker2d-medium-replay-v2_RORL_BASELINE_SEED0</strong>: <a href=\"https://wandb.ai/aimrl/MASTER_UNCERTAINTY/runs/9ft98zhu\" target=\"_blank\">https://wandb.ai/aimrl/MASTER_UNCERTAINTY/runs/9ft98zhu</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230417_205027-9ft98zhu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RUN_ID = \"9sflu2rl\" \n",
    "NEW_RUN_NAME = \"walker2d-medium-replay-v2_RORL_BASELINE_SEED0\"\n",
    "GROUP=\"14\"\n",
    "datalist, config  = get_data_from_run_full(RUN_ID)\n",
    "push_data_to_wandb(datalist,name= NEW_RUN_NAME,group=GROUP,config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "offrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4a2b216ecef5dfa7933c137a5a0c108b1507eef92d12a93e0c723d41094097b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
