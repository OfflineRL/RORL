{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n",
      "pybullet build time: Oct 11 2021 20:59:00\n"
     ]
    }
   ],
   "source": [
    "from email.mime import image\n",
    "from urllib.request import FancyURLopener\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import asdict\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import gtimer as gt\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "from lifelong_rl.torch.modules import LayerNorm\n",
    "from lifelong_rl.core import logger\n",
    "import lifelong_rl.torch.pytorch_util as ptu\n",
    "from scripts.get_config import get_rorl_config\n",
    "from experiment_utils.prepare_data import load_hdf5\n",
    "from lifelong_rl.core.logging.logging_setup import setup_logger\n",
    "from lifelong_rl.data_management.replay_buffers.env_replay_buffer import EnvReplayBuffer\n",
    "from lifelong_rl.data_management.replay_buffers.mujoco_replay_buffer import MujocoReplayBuffer\n",
    "from lifelong_rl.envs.env_processor import make_env\n",
    "from lifelong_rl.envs.env_utils import get_dim\n",
    "from lifelong_rl.samplers.data_collector.path_collector import MdpPathCollector, LatentPathCollector\n",
    "from lifelong_rl.samplers.data_collector.step_collector import MdpStepCollector, RFCollector, \\\n",
    "    GoalConditionedReplayStepCollector\n",
    "from experiment_utils.launch_experiment import launch_experiment\n",
    "from experiment_configs.algorithms.offline import get_offline_algorithm\n",
    "from lifelong_rl.samplers.utils.model_rollout_functions import policy\n",
    "from lifelong_rl.samplers.utils.rollout_functions import rollout_with_attack, rollout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data class of Namespace(base_log_dir='results', deterministic_backup=False, env_name='halfcheetah-medium-v2', epoch=3000, eta=-1.0, eval_attack=False, eval_attack_eps=0.01, eval_attack_mode='random', eval_no_training=False, exp_prefix='RORL', load_Qs='', load_config_type='attack', load_path='', log_to_tensorboard=False, max_q_backup=False, norm_input=True, num_qs=10, num_samples=20, plr=0.0003, policy_smooth_eps=0.0, policy_smooth_reg=1, q_ood_eps=0.0, q_ood_reg=0, q_ood_uncertainty_decay=0, q_ood_uncertainty_reg=0, q_ood_uncertainty_reg_min=0, q_smooth_eps=0.0, q_smooth_reg=0.005, q_smooth_tau=0.2, qlr=0.0003, reward_mean=False, reward_norm=False, reward_std=False, seed=0, shift_reward_minzero=False, use_cpu=False)\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    base_log_dir: str = 'results'\n",
    "    deterministic_backup: bool = False\n",
    "    env_name: str = 'halfcheetah-medium-v2'\n",
    "    epoch: int = 3000\n",
    "    eta: float = -1.0\n",
    "    eval_attack: bool = False\n",
    "    eval_attack_eps: float = 0.01\n",
    "    eval_attack_mode: str = 'random'\n",
    "    eval_no_training: bool = False\n",
    "    exp_prefix: str = 'RORL'\n",
    "    load_Qs: str = ''\n",
    "    load_config_type: str = 'attack'\n",
    "    load_path: str = ''\n",
    "    log_to_tensorboard: bool = False\n",
    "    max_q_backup: bool = False\n",
    "    norm_input: bool = True\n",
    "    num_qs: int = 10\n",
    "    num_samples: int = 20\n",
    "    plr: float = 0.0003\n",
    "    policy_smooth_eps: float = 0.0\n",
    "    policy_smooth_reg: int = 1\n",
    "    q_ood_eps: float = 0.0\n",
    "    q_ood_reg: int = 0\n",
    "    q_ood_uncertainty_decay: int = 0\n",
    "    q_ood_uncertainty_reg: int = 0\n",
    "    q_ood_uncertainty_reg_min: int = 0\n",
    "    q_smooth_eps: float = 0.0\n",
    "    q_smooth_reg: float = 0.005\n",
    "    q_smooth_tau: float = 0.2\n",
    "    qlr: float = 0.0003\n",
    "    reward_mean: bool = False\n",
    "    reward_norm: bool = False\n",
    "    reward_std: bool = False\n",
    "    seed: int = 0\n",
    "    shift_reward_minzero: bool = False\n",
    "    use_cpu: bool = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_from_args(args):\n",
    "    # Default parameters\n",
    "    variant = dict(\n",
    "        algorithm='',\n",
    "        collector_type='step',\n",
    "        env_name='hopper-random-v2',\n",
    "        env_kwargs=dict(),\n",
    "        replay_buffer_size=int(2e6),\n",
    "        reward_mean=False,  # added for easy config checking\n",
    "        reward_std=-1.0,  # added for easy config checking\n",
    "        policy_kwargs=dict(\n",
    "            layer_size=256,\n",
    "            num_q_layers=3,\n",
    "            num_p_layers=3,\n",
    "        ),\n",
    "        trainer_kwargs=dict(\n",
    "            discount=0.99,\n",
    "            soft_target_tau=5e-3,\n",
    "            policy_lr=3e-4,\n",
    "            qf_lr=3e-4,\n",
    "            use_automatic_entropy_tuning=True,\n",
    "            policy_eval_start=0,\n",
    "            num_qs=10,\n",
    "            target_update_period=1,\n",
    "            max_q_backup=False,\n",
    "            deterministic_backup=False,\n",
    "            eta=-1.0,\n",
    "        ),\n",
    "        offline_kwargs=dict(\n",
    "            num_epochs=3000,\n",
    "            num_eval_steps_per_epoch=1000,\n",
    "            num_trains_per_train_loop=1000,\n",
    "            max_path_length=1000, \n",
    "            batch_size=256,\n",
    "            save_snapshot_freq=500,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    experiment_kwargs = dict(\n",
    "        exp_postfix='',\n",
    "        use_gpu=True if torch.cuda.is_available() else False,\n",
    "        log_to_tensorboard=True,\n",
    "        base_log_dir=args.base_log_dir,\n",
    "    )\n",
    "\n",
    "    # Variant\n",
    "    variant['env_name'] = args.env_name\n",
    "    variant['seed'] = args.seed\n",
    "    variant['load_path'] = args.load_path\n",
    "    variant['load_Qs'] = args.load_Qs\n",
    "    variant['eval_no_training'] = args.eval_no_training\n",
    "    variant['eval_attack'] = args.eval_attack\n",
    "    variant['eval_attack_eps'] = args.eval_attack_eps\n",
    "    variant['eval_attack_mode'] = args.eval_attack_mode\n",
    "\n",
    "\n",
    "    variant['offline_kwargs']['num_epochs'] = args.epoch\n",
    "\n",
    "    # SAC-N\n",
    "    variant['trainer_kwargs']['policy_lr'] = args.plr\n",
    "    variant['trainer_kwargs']['qf_lr'] = args.qlr\n",
    "\n",
    "    variant['trainer_kwargs']['num_qs'] = args.num_qs\n",
    "    variant['trainer_kwargs']['max_q_backup'] = args.max_q_backup\n",
    "    variant['trainer_kwargs']['deterministic_backup'] = args.deterministic_backup\n",
    "\n",
    "    variant['reward_mean'] = args.reward_mean\n",
    "    variant['reward_std'] = args.reward_std\n",
    "    \n",
    "    # EDAC\n",
    "    variant['trainer_kwargs']['eta'] = args.eta\n",
    "\n",
    "    # smooth\n",
    "    if args.load_config_type != '':\n",
    "        rorl_config = get_rorl_config(args.env_name, args.load_config_type)\n",
    "        keys = ['num_samples', 'policy_smooth_eps', 'policy_smooth_reg', 'q_smooth_eps',\n",
    "                'q_smooth_reg', 'q_smooth_tau', 'q_ood_eps', 'q_ood_reg', 'q_ood_uncertainty_reg',\n",
    "                'q_ood_uncertainty_reg_min', 'q_ood_uncertainty_decay']\n",
    "        for key in keys:\n",
    "            variant['trainer_kwargs'][key] = rorl_config[key]\n",
    "    else:\n",
    "        variant['trainer_kwargs']['num_samples'] = args.num_samples\n",
    "        variant['trainer_kwargs']['policy_smooth_eps'] = args.policy_smooth_eps\n",
    "        variant['trainer_kwargs']['policy_smooth_reg'] = args.policy_smooth_reg\n",
    "        variant['trainer_kwargs']['q_smooth_eps'] = args.q_smooth_eps\n",
    "        variant['trainer_kwargs']['q_smooth_reg'] = args.q_smooth_reg\n",
    "        variant['trainer_kwargs']['q_smooth_tau'] = args.q_smooth_tau\n",
    "        variant['trainer_kwargs']['q_ood_eps'] = args.q_ood_eps\n",
    "        variant['trainer_kwargs']['q_ood_reg'] = args.q_ood_reg\n",
    "        variant['trainer_kwargs']['q_ood_uncertainty_reg'] = args.q_ood_uncertainty_reg\n",
    "        variant['trainer_kwargs']['q_ood_uncertainty_reg_min'] = args.q_ood_uncertainty_reg_min\n",
    "        variant['trainer_kwargs']['q_ood_uncertainty_decay'] = args.q_ood_uncertainty_decay\n",
    "\n",
    "    # experiment name\n",
    "    experiment_kwargs['exp_postfix'] = ''\n",
    "    \n",
    "    exp_postfix = args.exp_prefix + '_{}'.format(args.num_qs)\n",
    "    \n",
    "    # exp_postfix += '_plr{:.4f}_qlr{:.4f}'.format(args.plr, args.qlr)\n",
    "    if variant['trainer_kwargs']['max_q_backup']:\n",
    "        exp_postfix += '_maxq'\n",
    "    if variant['trainer_kwargs']['deterministic_backup']:\n",
    "        exp_postfix += '_detq'\n",
    "    if args.eta > 0:\n",
    "        exp_postfix += '_eta{:.2f}'.format(args.eta)\n",
    "    if args.reward_mean:\n",
    "        exp_postfix += '_mean'\n",
    "    if args.reward_std > 0:\n",
    "        exp_postfix += '_std'\n",
    "\n",
    "    experiment_kwargs['exp_postfix'] = exp_postfix\n",
    "\n",
    "    experiment_kwargs['data_args'] = {\n",
    "        'reward_mean': args.reward_mean,\n",
    "        'reward_std': args.reward_std,\n",
    "        'shift_reward_minzero': args.shift_reward_minzero,\n",
    "        'reward_norm': args.reward_norm,\n",
    "    }\n",
    "\n",
    "    ####### normalize input\n",
    "    variant['norm_input'] = args.norm_input\n",
    "    return(variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    algorithm: str\n",
    "    collector_type: str\n",
    "    env_name: str\n",
    "    env_kwargs: dict\n",
    "    replay_buffer_size: int\n",
    "    reward_mean: bool\n",
    "    reward_std: bool\n",
    "    policy_kwargs: dict\n",
    "    trainer_kwargs: dict\n",
    "    offline_kwargs: dict\n",
    "    seed: int\n",
    "    load_path: str\n",
    "    load_Qs: str\n",
    "    eval_no_training: bool\n",
    "    eval_attack: bool\n",
    "    eval_attack_eps: float\n",
    "    eval_attack_mode: str\n",
    "    norm_input: bool\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    algorithm='',\n",
    "    collector_type='step',\n",
    "    env_name='halfcheetah-medium-v2',\n",
    "    env_kwargs={},\n",
    "    replay_buffer_size=2000000,\n",
    "    reward_mean=False,\n",
    "    reward_std=False,\n",
    "    policy_kwargs={'layer_size': 256, 'num_q_layers': 3, 'num_p_layers': 3},\n",
    "    trainer_kwargs={\n",
    "        'discount': 0.99,\n",
    "        'soft_target_tau': 0.005,\n",
    "        'policy_lr': 0.0003,\n",
    "        'qf_lr': 0.0003,\n",
    "        'use_automatic_entropy_tuning': True,\n",
    "        'policy_eval_start': 0,\n",
    "        'num_qs': 10,\n",
    "        'target_update_period': 1,\n",
    "        'max_q_backup': False,\n",
    "        'deterministic_backup': False,\n",
    "        'eta': -1.0,\n",
    "        'num_samples': 20,\n",
    "        'policy_smooth_eps': 0.05,\n",
    "        'policy_smooth_reg': 1.0,\n",
    "        'q_smooth_eps': 0.03,\n",
    "        'q_smooth_reg': 0.0001,\n",
    "        'q_smooth_tau': 0.2,\n",
    "        'q_ood_eps': 0.0,\n",
    "        'q_ood_reg': 0.0,\n",
    "        'q_ood_uncertainty_reg': 0.0,\n",
    "        'q_ood_uncertainty_reg_min': 0.0,\n",
    "        'q_ood_uncertainty_decay': 0.0\n",
    "    },\n",
    "    offline_kwargs={\n",
    "        'num_epochs': 3000,\n",
    "        'num_eval_steps_per_epoch': 1000,\n",
    "        'num_trains_per_train_loop': 1000,\n",
    "        'max_path_length': 1000,\n",
    "        'batch_size': 256,\n",
    "        'save_snapshot_freq': 500\n",
    "    },\n",
    "    seed=0,\n",
    "    load_path='',\n",
    "    load_Qs='',\n",
    "    eval_no_training=False,\n",
    "    eval_attack=False,\n",
    "    eval_attack_eps=0.01,\n",
    "    eval_attack_mode='random',\n",
    "    norm_input=True\n",
    ")\n",
    "\n",
    "config_dict = asdict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelong_rl.models.networks import ParallelizedEnsembleFlattenMLP\n",
    "from lifelong_rl.policies.base.base import MakeDeterministic\n",
    "from lifelong_rl.policies.models.tanh_gaussian_policy import TanhGaussianPolicy\n",
    "from lifelong_rl.trainers.q_learning.sac import SACTrainer\n",
    "import lifelong_rl.util.pythonplusplus as ppp\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def get_config(\n",
    "        variant,\n",
    "        expl_env,\n",
    "        eval_env,\n",
    "        obs_dim,\n",
    "        action_dim,\n",
    "        replay_buffer,\n",
    "):\n",
    "    \"\"\"\n",
    "    Policy construction\n",
    "    \"\"\"\n",
    "\n",
    "    num_qs = variant['trainer_kwargs']['num_qs']\n",
    "    M = variant['policy_kwargs']['layer_size']\n",
    "    num_q_layers = variant['policy_kwargs']['num_q_layers']\n",
    "    num_p_layers = variant['policy_kwargs']['num_p_layers']\n",
    "\n",
    "    # normalization\n",
    "    norm_input = variant['norm_input']\n",
    "    obs_norm_mean, obs_norm_std = variant['normalization_info']['obs_mean'], variant['normalization_info']['obs_std']\n",
    "\n",
    "    qfs, target_qfs = ppp.group_init(\n",
    "        2,\n",
    "        ParallelizedEnsembleFlattenMLP,\n",
    "        ensemble_size=num_qs,\n",
    "        hidden_sizes=[M] * num_q_layers,\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        layer_norm=None,\n",
    "        norm_input=norm_input,\n",
    "        obs_norm_mean=obs_norm_mean,\n",
    "        obs_norm_std=obs_norm_std,\n",
    "    )\n",
    "\n",
    "    policy = TanhGaussianPolicy(\n",
    "        obs_dim=obs_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_sizes=[M] * num_p_layers,\n",
    "        layer_norm=None,\n",
    "        norm_input=norm_input,\n",
    "        obs_norm_mean=obs_norm_mean,\n",
    "        obs_norm_std=obs_norm_std,\n",
    "    )\n",
    "\n",
    "    trainer = SACTrainer(\n",
    "        env=eval_env,\n",
    "        policy=policy,\n",
    "        qfs=qfs,\n",
    "        target_qfs=target_qfs,\n",
    "        replay_buffer=replay_buffer,\n",
    "        norm_input=norm_input,\n",
    "        obs_std=obs_norm_std,\n",
    "        **variant['trainer_kwargs'],\n",
    "    )\n",
    "\n",
    "    if variant['load_path'] != '':\n",
    "        trainer.load_snapshot(variant['load_path'])\n",
    "    if variant['load_Qs'] != '':\n",
    "        trainer.load_qfs(variant['load_Qs'])\n",
    "    \"\"\"\n",
    "    Create config dict\n",
    "    \"\"\"\n",
    "\n",
    "    config = dict()\n",
    "    config.update(\n",
    "        dict(\n",
    "            trainer=trainer,\n",
    "            exploration_policy=policy,\n",
    "            evaluation_policy=MakeDeterministic(policy),\n",
    "            exploration_env=expl_env,\n",
    "            evaluation_env=eval_env,\n",
    "            replay_buffer=replay_buffer,\n",
    "            qfs=qfs,\n",
    "        ))\n",
    "    config['algorithm_kwargs'] = variant.get('algorithm_kwargs', dict())\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_experiment begin\n",
      "setup logger\n",
      "logging to: results/halfcheetah-medium-v2_Tue_Feb_28_21:53:36_2023_0\n",
      "2023-02-28 21:53:36.977903 KST | Variant:\n",
      "2023-02-28 21:53:36.979933 KST | {\n",
      "  \"algorithm\": \"\",\n",
      "  \"collector_type\": \"step\",\n",
      "  \"env_name\": \"halfcheetah-medium-v2\",\n",
      "  \"env_kwargs\": {},\n",
      "  \"replay_buffer_size\": 2000000,\n",
      "  \"reward_mean\": false,\n",
      "  \"reward_std\": false,\n",
      "  \"policy_kwargs\": {\n",
      "    \"layer_size\": 256,\n",
      "    \"num_q_layers\": 3,\n",
      "    \"num_p_layers\": 3\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.99,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"policy_lr\": 0.0003,\n",
      "    \"qf_lr\": 0.0003,\n",
      "    \"use_automatic_entropy_tuning\": true,\n",
      "    \"policy_eval_start\": 0,\n",
      "    \"num_qs\": 10,\n",
      "    \"target_update_period\": 1,\n",
      "    \"max_q_backup\": false,\n",
      "    \"deterministic_backup\": false,\n",
      "    \"eta\": -1.0,\n",
      "    \"num_samples\": 20,\n",
      "    \"policy_smooth_eps\": 0.05,\n",
      "    \"policy_smooth_reg\": 1.0,\n",
      "    \"q_smooth_eps\": 0.03,\n",
      "    \"q_smooth_reg\": 0.0001,\n",
      "    \"q_smooth_tau\": 0.2,\n",
      "    \"q_ood_eps\": 0.0,\n",
      "    \"q_ood_reg\": 0.0,\n",
      "    \"q_ood_uncertainty_reg\": 0.0,\n",
      "    \"q_ood_uncertainty_reg_min\": 0.0,\n",
      "    \"q_ood_uncertainty_decay\": 0.0\n",
      "  },\n",
      "  \"offline_kwargs\": {\n",
      "    \"num_epochs\": 3000,\n",
      "    \"num_eval_steps_per_epoch\": 1000,\n",
      "    \"num_trains_per_train_loop\": 1000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"batch_size\": 256,\n",
      "    \"save_snapshot_freq\": 500\n",
      "  },\n",
      "  \"seed\": 0,\n",
      "  \"load_path\": \"\",\n",
      "  \"load_Qs\": \"\",\n",
      "  \"eval_no_training\": false,\n",
      "  \"eval_attack\": false,\n",
      "  \"eval_attack_eps\": 0.01,\n",
      "  \"eval_attack_mode\": \"random\",\n",
      "  \"norm_input\": true\n",
      "}\n",
      "logger set!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 21/21 [00:02<00:00,  9.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rewards stats before preprocessing\n",
      "mean: 4.7702\n",
      "std: 1.2103\n",
      "max: 8.3267\n",
      "min: -2.8353\n",
      "\n",
      "Rewards stats after preprocessing\n",
      "mean: 4.7702\n",
      "std: 1.2103\n",
      "max: 8.3267\n",
      "min: -2.8353\n",
      "\n",
      "Replay buffer size : 999000\n",
      "obs dim            :  (999000, 17)\n",
      "action dim         :  (999000, 6)\n",
      "# terminals: 0\n",
      "Mean rewards       : 4.77\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_400869/2972639055.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0mget_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mget_offline_algorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_offline_algorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                     **experiment_kwargs)\n\u001b[0m",
      "\u001b[0;32m~/workspace/2023/common/RORL/experiment_utils/launch_experiment.py\u001b[0m in \u001b[0;36mlaunch_experiment\u001b[0;34m(variant, get_config, get_offline_algorithm, exp_postfix, use_gpu, log_to_tensorboard, base_log_dir, data_args)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mlog_to_tensorboard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_to_tensorboard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mbase_log_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_log_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mdata_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     )\n",
      "\u001b[0;32m~/workspace/2023/common/RORL/experiment_configs/base_experiment.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(variant, experiment_config, exp_postfix, use_gpu, log_to_tensorboard, base_log_dir, data_args)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_tabular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_timestamp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0moffline_algorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tabular_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/2023/common/RORL/lifelong_rl/core/rl_algorithms/offline/offline_rl_algorithm.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, start_epoch)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_end_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/2023/common/RORL/lifelong_rl/core/rl_algorithms/offline/offline_rl_algorithm.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m                                 self.batch_size, return_indices=True)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/2023/common/RORL/lifelong_rl/core/rl_algorithms/torch_rl_algorithm.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, np_batch, indices)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_train_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_to_pytorch_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_from_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_diagnostics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/2023/common/RORL/lifelong_rl/trainers/q_learning/sac.py\u001b[0m in \u001b[0;36mtrain_from_torch\u001b[0;34m(self, batch, indices)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mreparameterize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mreturn_log_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         )\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/offrl/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/2023/common/RORL/lifelong_rl/policies/models/tanh_gaussian_policy.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs, reparameterize, deterministic, return_log_prob, sample_n)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'layer_norm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/offrl/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    717\u001b[0m                 \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "experiment_kwargs = dict(\n",
    "    exp_postfix='',\n",
    "    use_gpu=True if torch.cuda.is_available() else False,\n",
    "    log_to_tensorboard=True,\n",
    "    base_log_dir='results',\n",
    ")\n",
    "\n",
    "launch_experiment(variant=config_dict, \n",
    "                    get_config=get_config,\n",
    "                    get_offline_algorithm=get_offline_algorithm,\n",
    "                    **experiment_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "offrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4a2b216ecef5dfa7933c137a5a0c108b1507eef92d12a93e0c723d41094097b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
