{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email.mime import image\n",
    "from urllib.request import FancyURLopener\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import asdict\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import gtimer as gt\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "from lifelong_rl.torch.modules import LayerNorm\n",
    "from lifelong_rl.core import logger\n",
    "import lifelong_rl.torch.pytorch_util as ptu\n",
    "from scripts.get_config import get_rorl_config\n",
    "from experiment_utils.prepare_data import load_hdf5\n",
    "from lifelong_rl.core.logging.logging_setup import setup_logger\n",
    "from lifelong_rl.data_management.replay_buffers.env_replay_buffer import EnvReplayBuffer\n",
    "from lifelong_rl.data_management.replay_buffers.mujoco_replay_buffer import MujocoReplayBuffer\n",
    "from lifelong_rl.envs.env_processor import make_env\n",
    "from lifelong_rl.envs.env_utils import get_dim\n",
    "from lifelong_rl.samplers.data_collector.path_collector import MdpPathCollector, LatentPathCollector\n",
    "from lifelong_rl.samplers.data_collector.step_collector import MdpStepCollector, RFCollector, \\\n",
    "    GoalConditionedReplayStepCollector\n",
    "from experiment_utils.launch_experiment import launch_experiment\n",
    "from experiment_configs.algorithms.offline import get_offline_algorithm\n",
    "from lifelong_rl.samplers.utils.model_rollout_functions import policy\n",
    "from lifelong_rl.samplers.utils.rollout_functions import rollout_with_attack, rollout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data class of Namespace(base_log_dir='results', deterministic_backup=False, env_name='halfcheetah-medium-v2', epoch=3000, eta=-1.0, eval_attack=False, eval_attack_eps=0.01, eval_attack_mode='random', eval_no_training=False, exp_prefix='RORL', load_Qs='', load_config_type='attack', load_path='', log_to_tensorboard=False, max_q_backup=False, norm_input=True, num_qs=10, num_samples=20, plr=0.0003, policy_smooth_eps=0.0, policy_smooth_reg=1, q_ood_eps=0.0, q_ood_reg=0, q_ood_uncertainty_decay=0, q_ood_uncertainty_reg=0, q_ood_uncertainty_reg_min=0, q_smooth_eps=0.0, q_smooth_reg=0.005, q_smooth_tau=0.2, qlr=0.0003, reward_mean=False, reward_norm=False, reward_std=False, seed=0, shift_reward_minzero=False, use_cpu=False)\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    base_log_dir: str = 'results'\n",
    "    deterministic_backup: bool = False\n",
    "    env_name: str = 'halfcheetah-medium-v2'\n",
    "    epoch: int = 3000\n",
    "    eta: float = -1.0\n",
    "    eval_attack: bool = False\n",
    "    eval_attack_eps: float = 0.01\n",
    "    eval_attack_mode: str = 'random'\n",
    "    eval_no_training: bool = False\n",
    "    exp_prefix: str = 'RORL'\n",
    "    load_Qs: str = ''\n",
    "    load_config_type: str = 'attack'\n",
    "    load_path: str = ''\n",
    "    log_to_tensorboard: bool = False\n",
    "    max_q_backup: bool = False\n",
    "    norm_input: bool = True\n",
    "    num_qs: int = 10\n",
    "    num_samples: int = 20\n",
    "    plr: float = 0.0003\n",
    "    policy_smooth_eps: float = 0.0\n",
    "    policy_smooth_reg: int = 1\n",
    "    q_ood_eps: float = 0.0\n",
    "    q_ood_reg: int = 0\n",
    "    q_ood_uncertainty_decay: int = 0\n",
    "    q_ood_uncertainty_reg: int = 0\n",
    "    q_ood_uncertainty_reg_min: int = 0\n",
    "    q_smooth_eps: float = 0.0\n",
    "    q_smooth_reg: float = 0.005\n",
    "    q_smooth_tau: float = 0.2\n",
    "    qlr: float = 0.0003\n",
    "    reward_mean: bool = False\n",
    "    reward_norm: bool = False\n",
    "    reward_std: bool = False\n",
    "    seed: int = 0\n",
    "    shift_reward_minzero: bool = False\n",
    "    use_cpu: bool = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_from_args(args):\n",
    "    # Default parameters\n",
    "    variant = dict(\n",
    "        algorithm='',\n",
    "        collector_type='step',\n",
    "        env_name='hopper-random-v2',\n",
    "        env_kwargs=dict(),\n",
    "        replay_buffer_size=int(2e6),\n",
    "        reward_mean=False,  # added for easy config checking\n",
    "        reward_std=-1.0,  # added for easy config checking\n",
    "        policy_kwargs=dict(\n",
    "            layer_size=256,\n",
    "            num_q_layers=3,\n",
    "            num_p_layers=3,\n",
    "        ),\n",
    "        trainer_kwargs=dict(\n",
    "            discount=0.99,\n",
    "            soft_target_tau=5e-3,\n",
    "            policy_lr=3e-4,\n",
    "            qf_lr=3e-4,\n",
    "            use_automatic_entropy_tuning=True,\n",
    "            policy_eval_start=0,\n",
    "            num_qs=10,\n",
    "            target_update_period=1,\n",
    "            max_q_backup=False,\n",
    "            deterministic_backup=False,\n",
    "            eta=-1.0,\n",
    "        ),\n",
    "        offline_kwargs=dict(\n",
    "            num_epochs=3000,\n",
    "            num_eval_steps_per_epoch=1000,\n",
    "            num_trains_per_train_loop=1000,\n",
    "            max_path_length=1000, \n",
    "            batch_size=256,\n",
    "            save_snapshot_freq=500,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    experiment_kwargs = dict(\n",
    "        exp_postfix='',\n",
    "        use_gpu=True if torch.cuda.is_available() else False,\n",
    "        log_to_tensorboard=True,\n",
    "        base_log_dir=args.base_log_dir,\n",
    "    )\n",
    "\n",
    "    # Variant\n",
    "    variant['env_name'] = args.env_name\n",
    "    variant['seed'] = args.seed\n",
    "    variant['load_path'] = args.load_path\n",
    "    variant['load_Qs'] = args.load_Qs\n",
    "    variant['eval_no_training'] = args.eval_no_training\n",
    "    variant['eval_attack'] = args.eval_attack\n",
    "    variant['eval_attack_eps'] = args.eval_attack_eps\n",
    "    variant['eval_attack_mode'] = args.eval_attack_mode\n",
    "\n",
    "\n",
    "    variant['offline_kwargs']['num_epochs'] = args.epoch\n",
    "\n",
    "    # SAC-N\n",
    "    variant['trainer_kwargs']['policy_lr'] = args.plr\n",
    "    variant['trainer_kwargs']['qf_lr'] = args.qlr\n",
    "\n",
    "    variant['trainer_kwargs']['num_qs'] = args.num_qs\n",
    "    variant['trainer_kwargs']['max_q_backup'] = args.max_q_backup\n",
    "    variant['trainer_kwargs']['deterministic_backup'] = args.deterministic_backup\n",
    "\n",
    "    variant['reward_mean'] = args.reward_mean\n",
    "    variant['reward_std'] = args.reward_std\n",
    "    \n",
    "    # EDAC\n",
    "    variant['trainer_kwargs']['eta'] = args.eta\n",
    "\n",
    "    # smooth\n",
    "    if args.load_config_type != '':\n",
    "        rorl_config = get_rorl_config(args.env_name, args.load_config_type)\n",
    "        keys = ['num_samples', 'policy_smooth_eps', 'policy_smooth_reg', 'q_smooth_eps',\n",
    "                'q_smooth_reg', 'q_smooth_tau', 'q_ood_eps', 'q_ood_reg', 'q_ood_uncertainty_reg',\n",
    "                'q_ood_uncertainty_reg_min', 'q_ood_uncertainty_decay']\n",
    "        for key in keys:\n",
    "            variant['trainer_kwargs'][key] = rorl_config[key]\n",
    "    else:\n",
    "        variant['trainer_kwargs']['num_samples'] = args.num_samples\n",
    "        variant['trainer_kwargs']['policy_smooth_eps'] = args.policy_smooth_eps\n",
    "        variant['trainer_kwargs']['policy_smooth_reg'] = args.policy_smooth_reg\n",
    "        variant['trainer_kwargs']['q_smooth_eps'] = args.q_smooth_eps\n",
    "        variant['trainer_kwargs']['q_smooth_reg'] = args.q_smooth_reg\n",
    "        variant['trainer_kwargs']['q_smooth_tau'] = args.q_smooth_tau\n",
    "        variant['trainer_kwargs']['q_ood_eps'] = args.q_ood_eps\n",
    "        variant['trainer_kwargs']['q_ood_reg'] = args.q_ood_reg\n",
    "        variant['trainer_kwargs']['q_ood_uncertainty_reg'] = args.q_ood_uncertainty_reg\n",
    "        variant['trainer_kwargs']['q_ood_uncertainty_reg_min'] = args.q_ood_uncertainty_reg_min\n",
    "        variant['trainer_kwargs']['q_ood_uncertainty_decay'] = args.q_ood_uncertainty_decay\n",
    "\n",
    "    # experiment name\n",
    "    experiment_kwargs['exp_postfix'] = ''\n",
    "    \n",
    "    exp_postfix = args.exp_prefix + '_{}'.format(args.num_qs)\n",
    "    \n",
    "    # exp_postfix += '_plr{:.4f}_qlr{:.4f}'.format(args.plr, args.qlr)\n",
    "    if variant['trainer_kwargs']['max_q_backup']:\n",
    "        exp_postfix += '_maxq'\n",
    "    if variant['trainer_kwargs']['deterministic_backup']:\n",
    "        exp_postfix += '_detq'\n",
    "    if args.eta > 0:\n",
    "        exp_postfix += '_eta{:.2f}'.format(args.eta)\n",
    "    if args.reward_mean:\n",
    "        exp_postfix += '_mean'\n",
    "    if args.reward_std > 0:\n",
    "        exp_postfix += '_std'\n",
    "\n",
    "    experiment_kwargs['exp_postfix'] = exp_postfix\n",
    "\n",
    "    experiment_kwargs['data_args'] = {\n",
    "        'reward_mean': args.reward_mean,\n",
    "        'reward_std': args.reward_std,\n",
    "        'shift_reward_minzero': args.shift_reward_minzero,\n",
    "        'reward_norm': args.reward_norm,\n",
    "    }\n",
    "\n",
    "    ####### normalize input\n",
    "    variant['norm_input'] = args.norm_input\n",
    "    return(variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    algorithm: str\n",
    "    collector_type: str\n",
    "    env_name: str\n",
    "    env_kwargs: dict\n",
    "    replay_buffer_size: int\n",
    "    reward_mean: bool\n",
    "    reward_std: bool\n",
    "    policy_kwargs: dict\n",
    "    trainer_kwargs: dict\n",
    "    offline_kwargs: dict\n",
    "    seed: int\n",
    "    load_path: str\n",
    "    load_Qs: str\n",
    "    eval_no_training: bool\n",
    "    eval_attack: bool\n",
    "    eval_attack_eps: float\n",
    "    eval_attack_mode: str\n",
    "    norm_input: bool\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    algorithm='',\n",
    "    collector_type='step',\n",
    "    env_name='halfcheetah-medium-v2',\n",
    "    env_kwargs={},\n",
    "    replay_buffer_size=2000000,\n",
    "    reward_mean=False,\n",
    "    reward_std=False,\n",
    "    policy_kwargs={'layer_size': 256, 'num_q_layers': 3, 'num_p_layers': 3},\n",
    "    trainer_kwargs={\n",
    "        'discount': 0.99,\n",
    "        'soft_target_tau': 0.005,\n",
    "        'policy_lr': 0.0003,\n",
    "        'qf_lr': 0.0003,\n",
    "        'use_automatic_entropy_tuning': True,\n",
    "        'policy_eval_start': 0,\n",
    "        'num_qs': 10,\n",
    "        'target_update_period': 1,\n",
    "        'max_q_backup': False,\n",
    "        'deterministic_backup': False,\n",
    "        'eta': -1.0,\n",
    "        'num_samples': 20,\n",
    "        'policy_smooth_eps': 0.05,\n",
    "        'policy_smooth_reg': 1.0,\n",
    "        'q_smooth_eps': 0.03,\n",
    "        'q_smooth_reg': 0.0001,\n",
    "        'q_smooth_tau': 0.2,\n",
    "        'q_ood_eps': 0.0,\n",
    "        'q_ood_reg': 0.0,\n",
    "        'q_ood_uncertainty_reg': 0.0,\n",
    "        'q_ood_uncertainty_reg_min': 0.0,\n",
    "        'q_ood_uncertainty_decay': 0.0\n",
    "    },\n",
    "    offline_kwargs={\n",
    "        'num_epochs': 3000,\n",
    "        'num_eval_steps_per_epoch': 1000,\n",
    "        'num_trains_per_train_loop': 1000,\n",
    "        'max_path_length': 1000,\n",
    "        'batch_size': 256,\n",
    "        'save_snapshot_freq': 500\n",
    "    },\n",
    "    seed=0,\n",
    "    load_path='',\n",
    "    load_Qs='',\n",
    "    eval_no_training=False,\n",
    "    eval_attack=False,\n",
    "    eval_attack_eps=0.01,\n",
    "    eval_attack_mode='random',\n",
    "    norm_input=True\n",
    ")\n",
    "\n",
    "config_dict = asdict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelong_rl.models.networks import ParallelizedEnsembleFlattenMLP\n",
    "from lifelong_rl.policies.base.base import MakeDeterministic\n",
    "from lifelong_rl.policies.models.tanh_gaussian_policy import TanhGaussianPolicy\n",
    "from lifelong_rl.trainers.q_learning.sac import SACTrainer\n",
    "import lifelong_rl.util.pythonplusplus as ppp\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def get_config(\n",
    "        variant,\n",
    "        expl_env,\n",
    "        eval_env,\n",
    "        obs_dim,\n",
    "        action_dim,\n",
    "        replay_buffer,\n",
    "):\n",
    "    \"\"\"\n",
    "    Policy construction\n",
    "    \"\"\"\n",
    "\n",
    "    num_qs = variant['trainer_kwargs']['num_qs']\n",
    "    M = variant['policy_kwargs']['layer_size']\n",
    "    num_q_layers = variant['policy_kwargs']['num_q_layers']\n",
    "    num_p_layers = variant['policy_kwargs']['num_p_layers']\n",
    "\n",
    "    # normalization\n",
    "    norm_input = variant['norm_input']\n",
    "    obs_norm_mean, obs_norm_std = variant['normalization_info']['obs_mean'], variant['normalization_info']['obs_std']\n",
    "\n",
    "    qfs, target_qfs = ppp.group_init(\n",
    "        2,\n",
    "        ParallelizedEnsembleFlattenMLP,\n",
    "        ensemble_size=num_qs,\n",
    "        hidden_sizes=[M] * num_q_layers,\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        layer_norm=None,\n",
    "        norm_input=norm_input,\n",
    "        obs_norm_mean=obs_norm_mean,\n",
    "        obs_norm_std=obs_norm_std,\n",
    "    )\n",
    "\n",
    "    policy = TanhGaussianPolicy(\n",
    "        obs_dim=obs_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_sizes=[M] * num_p_layers,\n",
    "        layer_norm=None,\n",
    "        norm_input=norm_input,\n",
    "        obs_norm_mean=obs_norm_mean,\n",
    "        obs_norm_std=obs_norm_std,\n",
    "    )\n",
    "\n",
    "    trainer = SACTrainer(\n",
    "        env=eval_env,\n",
    "        policy=policy,\n",
    "        qfs=qfs,\n",
    "        target_qfs=target_qfs,\n",
    "        replay_buffer=replay_buffer,\n",
    "        norm_input=norm_input,\n",
    "        obs_std=obs_norm_std,\n",
    "        **variant['trainer_kwargs'],\n",
    "    )\n",
    "\n",
    "    if variant['load_path'] != '':\n",
    "        trainer.load_snapshot(variant['load_path'])\n",
    "    if variant['load_Qs'] != '':\n",
    "        trainer.load_qfs(variant['load_Qs'])\n",
    "    \"\"\"\n",
    "    Create config dict\n",
    "    \"\"\"\n",
    "\n",
    "    config = dict()\n",
    "    config.update(\n",
    "        dict(\n",
    "            trainer=trainer,\n",
    "            exploration_policy=policy,\n",
    "            evaluation_policy=MakeDeterministic(policy),\n",
    "            exploration_env=expl_env,\n",
    "            evaluation_env=eval_env,\n",
    "            replay_buffer=replay_buffer,\n",
    "            qfs=qfs,\n",
    "        ))\n",
    "    config['algorithm_kwargs'] = variant.get('algorithm_kwargs', dict())\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment_kwargs = dict(\n",
    "    exp_postfix='',\n",
    "    use_gpu=True if torch.cuda.is_available() else False,\n",
    "    log_to_tensorboard=True,\n",
    "    base_log_dir='results',\n",
    ")\n",
    "\n",
    "launch_experiment(variant=config_dict, \n",
    "                    get_config=get_config,\n",
    "                    get_offline_algorithm=get_offline_algorithm,\n",
    "                    **experiment_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "offrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4a2b216ecef5dfa7933c137a5a0c108b1507eef92d12a93e0c723d41094097b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
